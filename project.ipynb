{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMVxsXNpo4CG7v+hIgzwWXG"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mf1pq4mxexIL",
        "outputId": "43731cc5-aa0e-4601-a8ce-8d53920351db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWacQf4lYdX-",
        "outputId": "f768a576-2b66-4098-c14d-7cee0fe440a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Oct 27 10:39:56 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P8    12W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CGMP8iQjE0YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTwlUToperiF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import pathlib\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXnDmXR7RDr2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_hub as hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if tf.test.gpu_device_name(): \n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "   print(\"Please turn on Colab GPU Runtime\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eu9LVGYrf7ks",
        "outputId": "8756b75c-9015-4fcc-b631-126ef7f372fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default GPU Device: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = pathlib.Path('/content/gdrive/MyDrive/CS4243/data')\n",
        "\n",
        "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
        "print(image_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acbiqYW2bF4X",
        "outputId": "7f83ca52-cca1-4b4b-a772-8b962adbe383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3960\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset_partitions_tf(ds, ds_size, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000, seed=42):\n",
        "    assert (train_split + test_split + val_split) == 1\n",
        "    \n",
        "    if shuffle:\n",
        "        # Specify seed to always have the same split distribution between runs\n",
        "        ds = ds.shuffle(shuffle_size, seed=seed)\n",
        "    \n",
        "    train_size = int(train_split * ds_size)\n",
        "    val_size = int(val_split * ds_size)\n",
        "    \n",
        "    train_ds = ds.take(train_size)    \n",
        "    val_ds = ds.skip(train_size).take(val_size)\n",
        "    test_ds = ds.skip(train_size).skip(val_size)\n",
        "    \n",
        "    return train_ds, val_ds, test_ds"
      ],
      "metadata": {
        "id": "7rJmstoY_WCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'), shuffle=False)\n",
        "list_ds = list_ds.shuffle(image_count, seed=42, reshuffle_each_iteration=False)"
      ],
      "metadata": {
        "id": "DarbmX6J_lte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = np.array(sorted([item.name for item in data_dir.glob('*')]))\n",
        "print(class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5L5jItHxBMm6",
        "outputId": "e072ab9c-0408-4db1-e0a6-3a73c6b891a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['carrying' 'normal' 'threat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, val_ds, test_ds = get_dataset_partitions_tf(list_ds, image_count, shuffle=False)"
      ],
      "metadata": {
        "id": "GO4wpE5EBPyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label(file_path):\n",
        "  # Convert the path to a list of path components\n",
        "  parts = tf.strings.split(file_path, os.path.sep)\n",
        "  # The second to last is the class-directory\n",
        "  one_hot = parts[-2] == class_names\n",
        "  # Integer encode the label\n",
        "  return tf.argmax(one_hot)\n",
        "\n",
        "def decode_img(img, image_size):\n",
        "  # Convert the compressed string to a 3D uint8 tensor\n",
        "  img = tf.io.decode_jpeg(img, channels=3)\n",
        "  # Resize the image to the desired size\n",
        "  return tf.image.resize(img, image_size)\n",
        "\n",
        "def parse_image(file_path, image_size):\n",
        "  label = get_label(file_path)\n",
        "  # Load the raw data from the file as a string\n",
        "  img = tf.io.read_file(file_path)\n",
        "  img = decode_img(img, image_size)\n",
        "  return img, label"
      ],
      "metadata": {
        "id": "I69nYbmWBxk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = [224, 224]\n",
        "batch_size = 32\n",
        "buffer_size = 100"
      ],
      "metadata": {
        "id": "V0GY0ehUD2SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
        "train_ds = train_ds.map(lambda x: parse_image(x, image_size), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.map(lambda x: parse_image(x, image_size), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.map(lambda x: parse_image(x, image_size), num_parallel_calls=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "PYLiUEbdDtrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalize = layers.Rescaling(1./255)\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.RandomRotation(0.2),\n",
        "])"
      ],
      "metadata": {
        "id": "_t9CcNBadf2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def prepare(ds, batch_size=32, shuffle=False, augment=False):\n",
        "  # Resize and rescale all datasets.\n",
        "  ds = ds.map(lambda x, y: (normalize(x), y), \n",
        "              num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(1000)\n",
        "\n",
        "  # Batch all datasets.\n",
        "  ds = ds.batch(batch_size)\n",
        "\n",
        "  # Use data augmentation only on the training set.\n",
        "  if augment:\n",
        "    ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), \n",
        "                num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "  # Use buffered prefetching on all datasets.\n",
        "  return ds.prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "2OO0iR91d-Xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = prepare(train_ds, shuffle=True, augment=True)\n",
        "val_ds = prepare(val_ds)\n",
        "test_ds = prepare(test_ds)"
      ],
      "metadata": {
        "id": "FMZ-yIvKedIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet(keras.Model):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(LeNet, self).__init__()\n",
        "    self.conv1 = layers.Conv2D(6, 5, padding='same', activation='relu')\n",
        "    self.pool1 = layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2))\n",
        "    self.conv2 = layers.Conv2D(16, 5, padding='same', activation='relu')\n",
        "    self.pool2 = layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2))\n",
        "    self.flatten = layers.Flatten()\n",
        "    self.d1 = layers.Dense(128, activation='relu')\n",
        "    self.d2 = layers.Dense(84, activation='relu')\n",
        "    self.final = layers.Dense(num_classes)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.pool1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.pool2(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    x = self.d2(x)\n",
        "    return self.final(x)\n",
        "\n",
        "# Create an instance of the model\n",
        "model = LeNet(num_classes=3)"
      ],
      "metadata": {
        "id": "3WR6AM_RejbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features, labels = next(iter(train_ds))\n",
        "\n",
        "print(features.shape)\n",
        "print(labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S95iGqGVqSVs",
        "outputId": "285f4c44-7885-41c6-b592-f61ed1bf6dfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 224, 224, 3)\n",
            "(32,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model(features)\n",
        "predictions[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEmbtgaiqfCR",
        "outputId": "42f9feee-401d-425b-8365-cb3d015291c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 3), dtype=float32, numpy=\n",
              "array([[ 5.3305387e-02,  1.4020781e-01, -1.8306246e-01],\n",
              "       [-9.9119730e-05,  3.0281678e-02, -8.5735612e-02],\n",
              "       [ 1.7102305e-02,  1.0672427e-01, -1.4694086e-01],\n",
              "       [ 4.6684887e-02,  7.5394258e-02, -1.3886884e-01],\n",
              "       [-1.5879069e-02,  1.5665153e-03, -4.2538531e-02]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model(features)\n",
        "predictions[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQARdB7dtcDR",
        "outputId": "acec2af8-dfd0-4097-e71a-f6a50c3b5571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 3), dtype=float32, numpy=\n",
              "array([[ 5.3305387e-02,  1.4020781e-01, -1.8306246e-01],\n",
              "       [-9.9119730e-05,  3.0281678e-02, -8.5735612e-02],\n",
              "       [ 1.7102305e-02,  1.0672427e-01, -1.4694086e-01],\n",
              "       [ 4.6684887e-02,  7.5394258e-02, -1.3886884e-01],\n",
              "       [-1.5879069e-02,  1.5665153e-03, -4.2538531e-02]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "train_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "val_metric = tf.keras.metrics.SparseCategoricalAccuracy()"
      ],
      "metadata": {
        "id": "patbvp0trg3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(x, y, loss_fn, optimizer, train_metric):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # training=True is needed only if there are layers with different\n",
        "        # behavior during training versus inference (e.g. Dropout).\n",
        "        preds = model(x, training=True)\n",
        "        loss_value = loss_fn(y, preds)\n",
        "\n",
        "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    train_metric.update_state(y, preds)\n",
        "\n",
        "    return loss_value"
      ],
      "metadata": {
        "id": "uBvknRTE10ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def val_step(x, y, loss_fn, val_metric):\n",
        "    val_preds = model(x, training=False)\n",
        "    val_loss = loss_fn(y, val_preds)\n",
        "    val_metric.update_state(y, val_preds)\n",
        "    return val_loss"
      ],
      "metadata": {
        "id": "hjzV_hDT3j2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_ds, val_ds, epochs, loss_fn, optimizer, train_metric, val_metric):\n",
        "    ## Note: Rerunning this cell uses the same model parameters\n",
        "\n",
        "    # Keep results for plotting\n",
        "    train_acc_per_epoch = []\n",
        "    val_acc_per_epoch = []\n",
        "    train_mean_loss = [] # Mean of each epoch\n",
        "    val_mean_loss = [] # Mean of each epoch\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "\n",
        "        start = time.time()\n",
        "        step = 0\n",
        "        # Training loop - using batches of 32\n",
        "        for x, y in train_ds:\n",
        "            # Optimize the model\n",
        "            loss = train_step(x, y, loss_fn, optimizer, train_metric)\n",
        "            train_losses.append(loss.numpy())\n",
        "\n",
        "            if step % 50 == 0:\n",
        "                print(f\"Training loss at epoch {epoch + 1}, step {step:d}: {float(loss):.5f}\")\n",
        "            \n",
        "            step += 1\n",
        "\n",
        "        for x, y in val_ds:\n",
        "            # training=False is needed only if there are layers with different\n",
        "            # behavior during training versus inference (e.g. Dropout).\n",
        "            loss = val_step(x, y)\n",
        "            val_losses.append(loss.numpy())\n",
        "        \n",
        "        # End epoch\n",
        "        train_acc = train_metric.result()\n",
        "        val_acc = val_metric.result()\n",
        "\n",
        "        train_mean_loss.append(np.mean(train_losses))\n",
        "        val_mean_loss.append(np.mean(val_losses))\n",
        "\n",
        "        train_acc_per_epoch.append(train_acc.numpy())\n",
        "        val_acc_per_epoch.append(val_acc.numpy())\n",
        "\n",
        "        train_metric.reset_states()\n",
        "        val_metric.reset_states()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}: train_loss: {np.mean(train_losses):.4f}, train_acc: {train_acc:.4f}, val_loss: {np.mean(val_losses):.4f}, val_acc: {val_acc:.4f}\")\n",
        "\n",
        "        print(f\"Time taken for this epoch: {time.time() - start:.2f}s\")\n",
        "        print('--' * 30)\n",
        "    history = (train_mean_loss, train_acc_per_epoch, val_mean_loss, val_acc_per_epoch)\n",
        "    return history"
      ],
      "metadata": {
        "id": "wl90ZOX-ubgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = train(model, train_ds, val_ds, 1, loss_fn, optimizer, train_metric, val_metric)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "HOVjEMV70Rsa",
        "outputId": "8e0b5c37-dd52-459b-9d3a-273ff50f3522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss at epoch 0, step 0: 1.13482\n",
            "Training loss at epoch 0, step 50: 1.06919\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-71e152da38f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_metric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-d512138d3446>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_ds, val_ds, epochs, loss_fn, optimizer, train_metric, val_metric)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Training loop - using batches of 32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;31m# Optimize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_metric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    764\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    750\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3012\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   3013\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IteratorGetNext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3014\u001b[0;31m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   3015\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3016\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nLz5bGITA5jR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}